{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78e9dcc",
   "metadata": {},
   "source": [
    "\n",
    "## Our objective is to find the sentiment of the movie reviews.\n",
    "### Proposed model - Clustering,Semi-Supervised learning and Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c6366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the reviews for as the input data\n",
    "X_train_df=pd.read_csv(\"X_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592d0129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shame, is a Swedish film in Swedish with Engli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I know it's rather unfair to comment on a movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Bread\" very sharply skewers the conventions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After reading tons of good reviews about this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During the Civil war a wounded union soldier h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>As a Pagan, I must say this movie has little i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>A lot of the comments seem to treat this film ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>I've only seen most of the series since I leav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>The \"all I have is 5 dollars and my wedding ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>When King Kong stripped her of her top in the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review\n",
       "0      Shame, is a Swedish film in Swedish with Engli...\n",
       "1      I know it's rather unfair to comment on a movi...\n",
       "2      \"Bread\" very sharply skewers the conventions o...\n",
       "3      After reading tons of good reviews about this ...\n",
       "4      During the Civil war a wounded union soldier h...\n",
       "...                                                  ...\n",
       "39995  As a Pagan, I must say this movie has little i...\n",
       "39996  A lot of the comments seem to treat this film ...\n",
       "39997  I've only seen most of the series since I leav...\n",
       "39998  The \"all I have is 5 dollars and my wedding ri...\n",
       "39999  When King Kong stripped her of her top in the ...\n",
       "\n",
       "[40000 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407c20ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for duplicates\n",
    "X_train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0cd6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  723,  3898,  4042,  4390,  5352,  5936,  6085,  6489,  7391,\n",
       "        7446,  7550,  7593,  7695,  7725,  8345,  9094,  9474,  9482,\n",
       "        9535,  9553,  9627,  9875, 10115, 10301, 10887, 10970, 11033,\n",
       "       11309, 11479, 12066, 12384, 12735, 12912, 13223, 13371, 13783,\n",
       "       14410, 14434, 14508, 14880, 14915, 15223, 15262, 15321, 15390,\n",
       "       15819, 16076, 16196, 16204, 16260, 16376, 16422, 16682, 16701,\n",
       "       17382, 17674, 17781, 18020, 18525, 18542, 18574, 18617, 18620,\n",
       "       19059, 19088, 19172, 19331, 19470, 19580, 19702, 20143, 20228,\n",
       "       20261, 20342, 20369, 20406, 20495, 20565, 20639, 20882, 20988,\n",
       "       21053, 21109, 21119, 21381, 21589, 21715, 21834, 21978, 22058,\n",
       "       22247, 22292, 22548, 22913, 22969, 23011, 23033, 23115, 23540,\n",
       "       23564, 23715, 23761, 23804, 23974, 24025, 24295, 24357, 24420,\n",
       "       24564, 24599, 24757, 24904, 25047, 25058, 25062, 25186, 25362,\n",
       "       25547, 25651, 25756, 25835, 25860, 25888, 25890, 25958, 26009,\n",
       "       26176, 26182, 26267, 26299, 26437, 26540, 26791, 26802, 26969,\n",
       "       27026, 27130, 27132, 27358, 27685, 27785, 27882, 28101, 28119,\n",
       "       28174, 28286, 28582, 28852, 28987, 28990, 29035, 29114, 29177,\n",
       "       29188, 29262, 29328, 29361, 29482, 29495, 29701, 29724, 29756,\n",
       "       29816, 29977, 30064, 30246, 30258, 30339, 30499, 30611, 30673,\n",
       "       30798, 30826, 30880, 30912, 30937, 30974, 31194, 31268, 31635,\n",
       "       31680, 31704, 31797, 31799, 31836, 31922, 32187, 32209, 32235,\n",
       "       32251, 32413, 32498, 32504, 32517, 32622, 32694, 32703, 32713,\n",
       "       32721, 32916, 32937, 33076, 33095, 33284, 33289, 33296, 33379,\n",
       "       33473, 33704, 33848, 33987, 34137, 34200, 34342, 34466, 34968,\n",
       "       35118, 35178, 35202, 35229, 35262, 35275, 35276, 35349, 35401,\n",
       "       35476, 35570, 35632, 35763, 35767, 35939, 35990, 35992, 36037,\n",
       "       36124, 36129, 36187, 36211, 36317, 36340, 36448, 36490, 36503,\n",
       "       36565, 36680, 36754, 36894, 36949, 36967, 36975, 37011, 37064,\n",
       "       37132, 37211, 37314, 37490, 37538, 37559, 37570, 37781, 37806,\n",
       "       37862, 37914, 37975, 38063, 38091, 38240, 38447, 38853, 38934,\n",
       "       38972, 39195, 39205, 39303, 39414, 39653, 39705, 39741, 39819,\n",
       "       39916, 39990], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the duplicate indices as they need to be removed from X and Y datasets simultaneously\n",
    "duplicated_indices = np.where(X_train_df.duplicated())[0]\n",
    "duplicated_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60516f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the duplicates from the input and labels\n",
    "y_train = y_train.drop(duplicated_indices).reset_index(drop=True)\n",
    "X_train_df=X_train_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da9cd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950169b",
   "metadata": {},
   "source": [
    "Basic cleaning of the textual data is performed using the below steps-\n",
    "1. Remove HTML tags using regular expressions.\n",
    "2. Convert text to lowercase.\n",
    "3. Remove punctuation using regular expressions.\n",
    "4. Tokenize text into individual words.\n",
    "5. Remove stop words.\n",
    "6. Lemmatize words.\n",
    "7. Join cleaned words into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab5a0ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Creating a function for cleaning the moview reviews\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    cleaned_text = re.sub(r'<.*?>|<br>', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    cleaned_text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    \n",
    "    # Tokenize text into individual words\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Assuming X_train_df is a pandas DataFrame with a 'text' column\n",
    "X_train_df['cleaned_text'] = X_train_df['review'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1558bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df['cleaned_text']=X_train_df['cleaned_text'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "541676ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shame, is a Swedish film in Swedish with Engli...</td>\n",
       "      <td>shame swedish film swedish english subtitle fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I know it's rather unfair to comment on a movi...</td>\n",
       "      <td>know rather unfair comment movie without seein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Bread\" very sharply skewers the conventions o...</td>\n",
       "      <td>bread sharply skewer convention horror movie g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After reading tons of good reviews about this ...</td>\n",
       "      <td>reading ton good review movie decided take spi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During the Civil war a wounded union soldier h...</td>\n",
       "      <td>civil war wounded union soldier hide isolated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39714</th>\n",
       "      <td>As a Pagan, I must say this movie has little i...</td>\n",
       "      <td>pagan must say movie little magickal significa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39715</th>\n",
       "      <td>A lot of the comments seem to treat this film ...</td>\n",
       "      <td>lot comment seem treat film baseball movie fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39716</th>\n",
       "      <td>I've only seen most of the series since I leav...</td>\n",
       "      <td>ive seen series since leave tv background nois...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39717</th>\n",
       "      <td>The \"all I have is 5 dollars and my wedding ri...</td>\n",
       "      <td>5 dollar wedding ring scene riot also guffawed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39718</th>\n",
       "      <td>When King Kong stripped her of her top in the ...</td>\n",
       "      <td>king kong stripped top 1976 remake breathless ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39719 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  \\\n",
       "0      Shame, is a Swedish film in Swedish with Engli...   \n",
       "1      I know it's rather unfair to comment on a movi...   \n",
       "2      \"Bread\" very sharply skewers the conventions o...   \n",
       "3      After reading tons of good reviews about this ...   \n",
       "4      During the Civil war a wounded union soldier h...   \n",
       "...                                                  ...   \n",
       "39714  As a Pagan, I must say this movie has little i...   \n",
       "39715  A lot of the comments seem to treat this film ...   \n",
       "39716  I've only seen most of the series since I leav...   \n",
       "39717  The \"all I have is 5 dollars and my wedding ri...   \n",
       "39718  When King Kong stripped her of her top in the ...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "0      shame swedish film swedish english subtitle fi...  \n",
       "1      know rather unfair comment movie without seein...  \n",
       "2      bread sharply skewer convention horror movie g...  \n",
       "3      reading ton good review movie decided take spi...  \n",
       "4      civil war wounded union soldier hide isolated ...  \n",
       "...                                                  ...  \n",
       "39714  pagan must say movie little magickal significa...  \n",
       "39715  lot comment seem treat film baseball movie fee...  \n",
       "39716  ive seen series since leave tv background nois...  \n",
       "39717  5 dollar wedding ring scene riot also guffawed...  \n",
       "39718  king kong stripped top 1976 remake breathless ...  \n",
       "\n",
       "[39719 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f87b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the reviews and their corresponding labels as test and trainning sets\n",
    "dX_train, dX_test, dy_train, dy_test = train_test_split(X_train_df[\"cleaned_text\"], \n",
    "                                                        y_train,\n",
    "                                                        test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c33af",
   "metadata": {},
   "source": [
    "### First model is just a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc679c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = 50\n",
    "pipeline_base = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_features=1000)),\n",
    "    ('scaler', MaxAbsScaler()),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the labeled training data\n",
    "pipeline_base.fit(dX_train[:labeled], dy_train[:labeled])\n",
    "\n",
    "# Calculate the accuracy on the test set\n",
    "pred_score= pipeline_base.score(dX_test, dy_test)\n",
    "print(f'Accuracy with 50 randomly labelled reviews are: {pred_score:.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2fa4e",
   "metadata": {},
   "source": [
    "#### Second approach is to cluster and label the points and perform semi-supervised learning followed by classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b70486ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Defining 25 clusters, and finding 3 points that are closest to these clusters and labelling them,this is done on the\\ntrain data and a classification model is fitted on it.\\nPost this approach we are using a classifier to detect the sentiment on the test data\\nHyper-parameter tuning is also performed on the best performing model'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Defining 25 clusters, and finding 3 points that are closest to these clusters and labelling them,this is done on the\n",
    "train data and a classification model is fitted on it.\n",
    "As a heuristic approach we choose the clusters which do not have duplicated index when checking for data points closest to\n",
    "the cluster centroid.\n",
    "Post this approach we are using a classifier to detect the sentiment on the test data\n",
    "Hyper-parameter tuning is also performed on the best performing model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e5d22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = 25\n",
    "\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "X_digits_dist = kmeans.fit_transform(X_train_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08ba8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels=pd.DataFrame(kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24e1b817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31775"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e48246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00238234, 0.9771361 , 0.9485253 , ..., 0.98092419, 1.04818809,\n",
       "        0.99162226],\n",
       "       [1.01454173, 0.9991134 , 0.9951155 , ..., 0.99656911, 1.08669672,\n",
       "        1.04037204],\n",
       "       [1.04035007, 1.0111528 , 0.99929466, ..., 1.01546194, 1.09263916,\n",
       "        1.0618453 ],\n",
       "       ...,\n",
       "       [0.99596378, 0.94108665, 0.98191245, ..., 0.96613751, 1.05357659,\n",
       "        0.98518532],\n",
       "       [1.03228739, 1.01463299, 0.98755656, ..., 1.01397382, 1.08555407,\n",
       "        1.0638152 ],\n",
       "       [1.01573189, 0.99142923, 0.97129412, ..., 0.98883224, 1.07050723,\n",
       "        1.03750447]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_digits_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "afc716cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_indices = np.argsort(X_digits_dist, axis=0)[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e00b51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18466, 11324, 15543, 27924, 22911, 19547, 30151, 18877, 20157,\n",
       "        19176, 26235, 11660, 15895, 14975, 25273, 19896, 14397, 25253,\n",
       "        15805,  8445, 13716, 28940, 11660, 20643,  1165],\n",
       "       [19216,  6767, 19397,  4013, 17992, 17623, 31659, 28358,   641,\n",
       "        19620,  5504, 17376, 20048, 12063, 24803,  6998, 16478, 29729,\n",
       "        31039, 15175,  6444, 23699, 11808, 19547, 29576],\n",
       "       [ 4147, 24995, 11152, 19232,   599, 19925, 14748, 20758,  1581,\n",
       "        18404, 13318,   254,  2092, 21410, 29106, 29809,   559, 24281,\n",
       "        19655, 16677, 17008, 13295,  5199, 14828,  6129]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a91bba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18466, 11324, 15543, 27924, 22911, 19547, 30151, 18877, 20157,\n",
       "       19176, 26235, 11660, 15895, 14975, 25273, 19896, 14397, 25253,\n",
       "       15805,  8445, 13716, 28940, 11660, 20643,  1165, 19216,  6767,\n",
       "       19397,  4013, 17992, 17623, 31659, 28358,   641, 19620,  5504,\n",
       "       17376, 20048, 12063, 24803,  6998, 16478, 29729, 31039, 15175,\n",
       "        6444, 23699, 11808, 19547, 29576,  4147, 24995, 11152, 19232,\n",
       "         599, 19925, 14748, 20758,  1581, 18404, 13318,   254,  2092,\n",
       "       21410, 29106, 29809,   559, 24281, 19655, 16677, 17008, 13295,\n",
       "        5199, 14828,  6129], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_indices_flattened = nearest_indices.flatten()\n",
    "nearest_indices_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "209425c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_representative_digits = X_train_vectorized[nearest_indices_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73d3a2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<75x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8691 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_representative_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e775ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_train_ser=dy_train[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e287234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0            40\n",
       "1            35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_train.iloc[nearest_indices_flattened].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a307219",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_representative_digits = dy_train.iloc[nearest_indices_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcd2c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with only 50 representative training examples: 66.59%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_reg = SVC()\n",
    "svc_reg = svc_reg.fit(X_representative_digits, y_representative_digits)\n",
    "\n",
    "new_scr = svc_reg.score(X_test_vectorized, dy_test)\n",
    "print(f'Accuracy with only 50 representative training examples: {new_scr:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f8df573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.83      0.71      3989\n",
      "           1       0.74      0.50      0.60      3955\n",
      "\n",
      "    accuracy                           0.67      7944\n",
      "   macro avg       0.68      0.67      0.66      7944\n",
      "weighted avg       0.68      0.67      0.66      7944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have already trained and evaluated the logistic regression model\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_1 = svc_reg.predict(X_test_vectorized)\n",
    "\n",
    "# Generate the classification report\n",
    "classification_rep = classification_report(dy_test, y_pred_1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87fc633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with only 50 representative training examples: 67.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_1 = LogisticRegression()\n",
    "log_reg_1 = log_reg_1.fit(X_representative_digits, y_representative_digits)\n",
    "\n",
    "new_lr_scr = log_reg_1.score(X_test_vectorized, dy_test)\n",
    "print(f'Accuracy with only 50 representative training examples: {new_lr_scr:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44f056a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_propagated = np.empty(len(dX_train), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53135250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1017843008,        541, 1022384224, ...,          0,          0,\n",
       "                0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train_propagated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0568ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "Name: 16651, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_representative_digits.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92701532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with propagation: 58.40%\n"
     ]
    }
   ],
   "source": [
    "#for i in range(k):\n",
    "#    y_train_propagated[kmeans.labels_ == i] = y_representative_digits.iloc[i]\n",
    "#\n",
    "#rand_final = RandomForestClassifier()\n",
    "#rand_final = rand_final.fit(X_train_vectorized, y_train_propagated)\n",
    "#\n",
    "#new_scr = rand_final.score(X_test_vectorized, dy_test)\n",
    "#print(f'Accuracy with propagation: {new_scr:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d4897eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prod=pd.read_csv(\"y_final.csv\")\n",
    "X_prod=pd.read_csv(\"X_final.csv\")\n",
    "X_prod['cleaned_text'] = X_prod['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07762ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec_prod = vectorizer.transform(X_prod['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b278f3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72      5000\n",
      "           1       0.75      0.57      0.65      5000\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.70      0.69      0.69     10000\n",
      "weighted avg       0.70      0.69      0.69     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have already trained and evaluated the logistic regression model\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prod = rf_10.predict(X_vec_prod)\n",
    "\n",
    "# Generate the classification report\n",
    "classification_rep_prod = classification_report(y_prod, y_pred_prod)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_rep_prod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "97dce94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "def k_means(X_train_vectorized):\n",
    "    k = 25\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    X_digits_dist = kmeans.fit_transform(X_train_vectorized)\n",
    "    kmeans_labels = pd.DataFrame(kmeans.labels_)\n",
    "    nearest_indices = np.argsort(X_digits_dist, axis=0)[1:4]\n",
    "    nearest_indices_flattened = nearest_indices.flatten()\n",
    "    X_representative_digits = X_train_vectorized[nearest_indices_flattened]\n",
    "    y_representative_digits = dy_train.iloc[nearest_indices_flattened]\n",
    "    y_representative_digits=y_representative_digits.values.ravel()\n",
    "    return X_representative_digits, y_representative_digits\n",
    "\n",
    "\n",
    "class KMeansTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k_means):\n",
    "        self.k_means = k_means\n",
    "\n",
    "    def fit(self, X_train_vectorized, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_train_vectorized):\n",
    "        X_representative_digits, y_representative_digits = self.k_means(X_train_vectorized)\n",
    "        return X_representative_digits, y_representative_digits\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000)),\n",
    "    ('scalar', MaxAbsScaler()),\n",
    "    ('kmeans', KMeansTransformer(k_means)),])\n",
    "\n",
    "pipeline1 = pipeline.fit(dX_train, dy_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9576e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_10 = RandomForestClassifier(n_estimators=1000)\n",
    "rf_10 = rf_10.fit(X_representative_digits, y_representative_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5c34b2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with only 75 representative training examples: 68.47%\n"
     ]
    }
   ],
   "source": [
    "rf_score = rf_10.score(X_test_vectorized, dy_test)\n",
    "print(f'Accuracy with only 75 representative training examples: {new_scr:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "def k_means(X_train_vectorized):\n",
    "    k = 25\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    X_digits_dist = kmeans.fit_transform(X_train_vectorized)\n",
    "    kmeans_labels = pd.DataFrame(kmeans.labels_)\n",
    "    nearest_indices = np.argsort(X_digits_dist, axis=0)[1:4]\n",
    "    nearest_indices_flattened = nearest_indices.flatten()\n",
    "    X_representative_digits = X_train_vectorized[nearest_indices_flattened]\n",
    "    y_representative_digits = dy_train.iloc[nearest_indices_flattened]\n",
    "    y_representative_digits=y_representative_digits.values.ravel()\n",
    "    return X_representative_digits, y_representative_digits\n",
    "class KMeansTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k_means):\n",
    "        self.k_means = k_means\n",
    "\n",
    "    def fit(self, X_train_vectorized, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_train_vectorized):\n",
    "        X_representative_digits, y_representative_digits = self.k_means(X_train_vectorized)\n",
    "        return X_representative_digits, y_representative_digits\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000)),\n",
    "    ('scalar', MaxAbsScaler()),\n",
    "    ('kmeans', KMeansTransformer(k_means)),\n",
    "    ('rf',RandomForestClassifier(n_estimators=1000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10,15]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05571172",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(dX_train, dy_train)\n",
    "accuracy = best_model.score(dX_test, dy_test)\n",
    "print(\"The accuracy of random forest is\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cebb8",
   "metadata": {},
   "source": [
    "### Conclusion -\n",
    "1. In comparison to using just a random set of labels to train the model, using it alongside a unsupervised technique will enhance the models performance.\n",
    "3. Using a Dimentionality reduction such as TruncatedSVD is not suggested since the explained variance is very less, hence not helping much with the computation.\n",
    "4. The best combination of the model that worked the best in this case is-\n",
    "- k-means with 25 clusters and 3 points closest to each of the cluster as labels of the training data\n",
    "- Using Random Forest as opposed to using Logistic Regression and SVM worked better\n",
    "\n",
    "Further enhancements -  Another proposal that can be tried out is label propogation along with the clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6f45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
